{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.5 - Student Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This lab is a continuation of the guided labs in Module 3. \n",
    "\n",
    "In this lab, you will deploy a trained model and perform a prediction against the model. You will then delete the endpoint and perform a batch transform on the test dataset.\n",
    "\n",
    "\n",
    "## Introduction to the business scenario\n",
    "\n",
    "You work for a healthcare provider, and want to improve the detection of abnormalities in orthopedic patients. \n",
    "\n",
    "You are tasked with solving this problem by using machine learning (ML). You have access to a dataset that contains six biomechanical features and a target of *normal* or *abnormal*. You can use this dataset to train an ML model to predict if a patient will have an abnormality.\n",
    "\n",
    "\n",
    "## About this dataset\n",
    "\n",
    "This biomedical dataset was built by Dr. Henrique da Mota during a medical residence period in the Group of Applied Research in Orthopaedics (GARO) of the Centre Médico-Chirurgical de Réadaptation des Massues, Lyon, France. The data has been organized in two different, but related, classification tasks. \n",
    "\n",
    "The first task consists in classifying patients as belonging to one of three categories: \n",
    "\n",
    "- *Normal* (100 patients)\n",
    "- *Disk Hernia* (60 patients)\n",
    "- *Spondylolisthesis* (150 patients)\n",
    "\n",
    "For the second task, the categories *Disk Hernia* and *Spondylolisthesis* were merged into a single category that is labeled as *abnormal*. Thus, the second task consists in classifying patients as belonging to one of two categories: *Normal* (100 patients) or *Abnormal* (210 patients).\n",
    "\n",
    "\n",
    "## Attribute information\n",
    "\n",
    "Each patient is represented in the dataset by six biomechanical attributes that are derived from the shape and orientation of the pelvis and lumbar spine (in this order): \n",
    "\n",
    "- Pelvic incidence\n",
    "- Pelvic tilt\n",
    "- Lumbar lordosis angle\n",
    "- Sacral slope\n",
    "- Pelvic radius\n",
    "- Grade of spondylolisthesis\n",
    "\n",
    "The following convention is used for the class labels: \n",
    "- DH (Disk Hernia)\n",
    "- Spondylolisthesis (SL)\n",
    "- Normal (NO) \n",
    "- Abnormal (AB)\n",
    "\n",
    "For more information about this dataset, see the [Vertebral Column dataset webpage](http://archive.ics.uci.edu/ml/datasets/Vertebral+Column).\n",
    "\n",
    "\n",
    "## Dataset attributions\n",
    "\n",
    "This dataset was obtained from:\n",
    "Dua, D. and Graff, C. (2019). UCI Machine Learning Repository (http://archive.ics.uci.edu/ml). Irvine, CA: University of California, School of Information and Computer Science.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab setup\n",
    "\n",
    "Because this solution is split across several labs in the module, you run the following cells so that you can load the data and train the model to be deployed.\n",
    "\n",
    "**Note:** The setup can take up to 5 minutes to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data\n",
    "\n",
    "By running the following cells, the data will be imported and ready for use. \n",
    "\n",
    "**Note:** The following cells represent the key steps in the previous labs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket='c94466a2114432l5130264t1w295847703765-labbucket-1s9m53vwziobx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import warnings, requests, zipfile, io\n",
    "warnings.simplefilter('ignore')\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (uriginal): https://drive.google.com/uc?id=1Qyav9ORUYqGXN-S7nx8zrYVxAtYR97VE\n",
      "From (redirected): https://drive.google.com/uc?id=1Qyav9ORUYqGXN-S7nx8zrYVxAtYR97VE&confirm=t&uuid=f50f4517-7c70-408e-ab23-03a389f01936\n",
      "To: /home/ec2-user/SageMaker/en_us/combined_csv_v1.csv\n",
      "100%|██████████| 318M/318M [00:03<00:00, 96.3MB/s] \n",
      "Downloading...\n",
      "From (uriginal): https://drive.google.com/uc?id=1b5dA5u_VnZP1ZjQxmhigbuIOfmqjx70x\n",
      "From (redirected): https://drive.google.com/uc?id=1b5dA5u_VnZP1ZjQxmhigbuIOfmqjx70x&confirm=t&uuid=bc28147c-7f06-4574-aeb4-9535fe9c0b68\n",
      "To: /home/ec2-user/SageMaker/en_us/combined_csv_v2.csv\n",
      "100%|██████████| 384M/384M [00:03<00:00, 103MB/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files downloaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gdown\n",
    "#Load the dataset from my google drive\n",
    "#https://drive.google.com/file/d/1Qyav9ORUYqGXN-S7nx8zrYVxAtYR97VE/view?usp=sharing\n",
    "#https://drive.google.com/file/d/1b5dA5u_VnZP1ZjQxmhigbuIOfmqjx70x/view?usp=drive_link\n",
    "\n",
    "# Define a dictionary of file names and their corresponding file IDs\n",
    "file_ids = {\n",
    "    'combined_csv_v1.csv': '1Qyav9ORUYqGXN-S7nx8zrYVxAtYR97VE',\n",
    "    'combined_csv_v2.csv': '1b5dA5u_VnZP1ZjQxmhigbuIOfmqjx70x',\n",
    "}\n",
    "\n",
    "# Define the destination folder where you want to save the files\n",
    "destination_folder = './'\n",
    "\n",
    "# Download the files\n",
    "for file_name, file_id in file_ids.items():\n",
    "    url = f'https://drive.google.com/uc?id={file_id}'\n",
    "    output = f'{destination_folder}/{file_name}'\n",
    "    gdown.download(url, output, quiet=False)\n",
    "\n",
    "print('Files downloaded successfully.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_optimized_csv(filename, target_column='target'):\n",
    "    # Read a small sample to infer data types\n",
    "    small_sample = pd.read_csv(filename, nrows=1000)\n",
    "\n",
    "    # Identify columns to be converted to bool (binary columns)\n",
    "    bool_columns = [col for col in small_sample.columns if col not in [target_column, 'Distance'] and small_sample[col].nunique() == 2]\n",
    "\n",
    "    # Create a dictionary with specified data types\n",
    "    column_types = {col: 'bool' for col in bool_columns}\n",
    "    column_types['Distance'] = 'float32'\n",
    "    column_types[target_column] = 'float32'\n",
    "\n",
    "    # Read the full CSV with optimized data types\n",
    "    df = pd.read_csv(filename, dtype=column_types)\n",
    "\n",
    "    return df\n",
    "data_v1 = pd.read_csv(\"combined_csv_v1.csv\").sample(frac=0.5)  # samples 50% of the data\n",
    "data_v2 = pd.read_csv(\"combined_csv_v2.csv\").sample(frac=0.5)  # samples 50% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n",
      "INFO:sagemaker.image_uris:Defaulting to only supported image scope: cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2023-11-01-21-42-39-081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2023-11-01 21:42:39 Starting - Starting the training job......\n",
      "2023-11-01 21:43:15 Starting - Preparing the instances for training............\n",
      "2023-11-01 21:44:24 Downloading - Downloading input data......\n",
      "2023-11-01 21:44:59 Training - Downloading the training image.......\n",
      "2023-11-01 21:45:40 Training - Training image download completed. Training in progress................\n",
      "2023-11-01 21:47:01 Uploading - Uploading generated training model...\n",
      "2023-11-01 21:47:17 Completed - Training job completed\n",
      "ready for hosting!\n"
     ]
    }
   ],
   "source": [
    "train, test_and_validate = train_test_split(data_v1, test_size=0.2, random_state=42, stratify=data_v1['target'])\n",
    "test, validate = train_test_split(test_and_validate, test_size=0.5, random_state=42, stratify=test_and_validate['target'])\n",
    "\n",
    "prefix='lab3'\n",
    "\n",
    "train_file='vertebral_train.csv'\n",
    "test_file='vertebral_test.csv'\n",
    "validate_file='vertebral_validate.csv'\n",
    "\n",
    "s3_resource = boto3.Session().resource('s3')\n",
    "def upload_s3_csv(filename, folder, dataframe):\n",
    "    csv_buffer = io.StringIO()\n",
    "    dataframe.to_csv(csv_buffer, header=False, index=False )\n",
    "    s3_resource.Bucket(bucket).Object(os.path.join(prefix, folder, filename)).put(Body=csv_buffer.getvalue())\n",
    "\n",
    "upload_s3_csv(train_file, 'train', train)\n",
    "upload_s3_csv(test_file, 'test', test)\n",
    "upload_s3_csv(validate_file, 'validate', validate)\n",
    "\n",
    "container = retrieve('xgboost',boto3.Session().region_name,'1.0-1')\n",
    "\n",
    "hyperparams={\"num_round\":\"42\",\n",
    "             \"eval_metric\": \"auc\",\n",
    "             \"objective\": \"binary:logistic\"}\n",
    "\n",
    "s3_output_location=\"s3://{}/{}/output/\".format(bucket,prefix)\n",
    "xgb_model=sagemaker.estimator.Estimator(container,\n",
    "                                       sagemaker.get_execution_role(),\n",
    "                                       instance_count=1,\n",
    "                                       instance_type='ml.m4.xlarge',\n",
    "                                       output_path=s3_output_location,\n",
    "                                        hyperparameters=hyperparams,\n",
    "                                        sagemaker_session=sagemaker.Session())\n",
    "\n",
    "train_channel = sagemaker.inputs.TrainingInput(\n",
    "    \"s3://{}/{}/train/\".format(bucket,prefix,train_file),\n",
    "    content_type='text/csv')\n",
    "\n",
    "validate_channel = sagemaker.inputs.TrainingInput(\n",
    "    \"s3://{}/{}/validate/\".format(bucket,prefix,validate_file),\n",
    "    content_type='text/csv')\n",
    "\n",
    "data_channels = {'train': train_channel, 'validation': validate_channel}\n",
    "\n",
    "xgb_model.fit(inputs=data_channels, logs=False)\n",
    "\n",
    "print('ready for hosting!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Hosting the model\n",
    "\n",
    "Now that you have a trained model, you can host it by using Amazon SageMaker hosting services.\n",
    "\n",
    "The first step is to deploy the model. Because you have a model object, *xgb_model*, you can use the **deploy** method. For this lab, you will use a single ml.m4.xlarge instance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2023-11-01-21-47-21-993\n",
      "INFO:sagemaker:Creating endpoint-config with name sagemaker-xgboost-2023-11-01-21-47-21-993\n",
      "INFO:sagemaker:Creating endpoint with name sagemaker-xgboost-2023-11-01-21-47-21-993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    }
   ],
   "source": [
    "xgb_predictor = xgb_model.deploy(initial_instance_count=1,\n",
    "                serializer = sagemaker.serializers.CSVSerializer(),\n",
    "                instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Performing predictions\n",
    "\n",
    "Now that you have a deployed model, you will run some predictions.\n",
    "\n",
    "First, review the test data and re-familiarize yourself with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have 31 instances, with seven attributes. The first five instances are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You don't need to include the target value (class). This predictor can take data in the comma-separated values (CSV) format. You can thus get the first row *without the class column* by using the following code:\n",
    "\n",
    "`test.iloc[:1,1:]` \n",
    "\n",
    "The **iloc** function takes parameters of [*rows*,*cols*]\n",
    "\n",
    "To only get the first row, use `0:1`. If you want to get row 2, you could use `1:2`.\n",
    "\n",
    "To get all columns *except* the first column (*col 0*), use `1:`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = test.iloc[0:1,1:]\n",
    "row.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can convert this to a comma-separated values (CSV) file, and store it in a string buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_X_csv_buffer = io.StringIO()\n",
    "row.to_csv(batch_X_csv_buffer, header=False, index=False)\n",
    "test_row = batch_X_csv_buffer.getvalue()\n",
    "print(test_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the data to perform a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.predict(test_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result you get isn't a *0* or a *1*. Instead, you get a *probability score*. You can apply some conditional logic to the probability score to determine if the answer should be presented as a 0 or a 1. You will work with this process when you do batch predictions.\n",
    "\n",
    "For now, compare the result with the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Is the prediction accurate?\n",
    "\n",
    "**Challenge task:** Update the previous code to send the second row of the dataset. Are those predictions correct? Try this task with a few other rows.\n",
    "\n",
    "It can be tedious to send these rows one at a time. You could write a function to submit these values in a batch, but SageMaker already has a batch capability. You will examine that feature next. However, before you do, you will terminate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Terminating the deployed model\n",
    "\n",
    "To delete the endpoint, use the **delete_endpoint** function on the predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Performing a batch transform\n",
    "\n",
    "When you are in the training-testing-feature engineering cycle, you want to test your holdout or test sets against the model. You can then use those results to calculate metrics. You could deploy an endpoint as you did earlier, but then you must remember to delete the endpoint. However, there is a more efficient way.\n",
    "\n",
    "You can use the transformer method of the model to get a transformer object. You can then use the transform method of this object to perform a prediction on the entire test dataset. SageMaker will: \n",
    "\n",
    "- Spin up an instance with the model\n",
    "- Perform a prediction on all the input values\n",
    "- Write those values to Amazon Simple Storage Service (Amazon S3) \n",
    "- Finally, terminate the instance\n",
    "\n",
    "You will start by turning your data into a CSV file that the transformer object can take as input. This time, you will use **iloc** to get all the rows, and all columns *except* the first column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_X = test.iloc[:,1:];\n",
    "batch_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, write your data to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_X_file='batch-in.csv'\n",
    "upload_s3_csv(batch_X_file, 'batch-in', batch_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, before you perform a transform, configure your transformer with the input file, output location, and instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_output = \"s3://{}/{}/batch-out/\".format(bucket,prefix)\n",
    "batch_input = \"s3://{}/{}/batch-in/{}\".format(bucket,prefix,batch_X_file)\n",
    "\n",
    "xgb_transformer = xgb_model.transformer(instance_count=1,\n",
    "                                       instance_type='ml.m4.xlarge',\n",
    "                                       strategy='MultiRecord',\n",
    "                                       assemble_with='Line',\n",
    "                                       output_path=batch_output)\n",
    "\n",
    "xgb_transformer.transform(data=batch_input,\n",
    "                         data_type='S3Prefix',\n",
    "                         content_type='text/csv',\n",
    "                         split_type='Line')\n",
    "xgb_transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the transform completes, you can download the results from Amazon S3 and compare them with the input.\n",
    "\n",
    "First, download the output from Amazon S3 and load it into a pandas DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "obj = s3.get_object(Bucket=bucket, Key=\"{}/batch-out/{}\".format(prefix,'batch-in.csv.out'))\n",
    "target_predicted = pd.read_csv(io.BytesIO(obj['Body'].read()),sep=',',names=['class'])\n",
    "target_predicted.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use a function to convert the probabilty into either a *0* or a *1*.\n",
    "\n",
    "The first table output will be the *predicted values*, and the second table output is the *original test data*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_convert(x):\n",
    "    threshold = 0.65\n",
    "    if x > threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "target_predicted['binary'] = target_predicted['class'].apply(binary_convert)\n",
    "\n",
    "print(target_predicted.head(10))\n",
    "test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The *threshold* in the **binary_convert** function is set to *.65*.\n",
    "\n",
    "**Challenge task:** Experiment with changing the value of the threshold. Does it impact the results?\n",
    "\n",
    "**Note:** The initial model might not be good. You will generate some metrics in the next lab, before you tune the model in the final lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "\n",
    "You have completed this lab, and you can now end the lab by following the lab guide instructions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
